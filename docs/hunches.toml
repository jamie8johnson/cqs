# Hunches - soft observations indexed by cqs
# These surface in search results when semantically relevant.
# Format: see CLAUDE.md for schema

[[hunch]]
date = "2026-02-01"
title = "cqs is Tears, not code search"
severity = "high"
confidence = "high"
resolution = "open"
mentions = ["README.md", "CLAUDE.md"]
description = """
cqs marketed as 'semantic code search' but actual value is context persistence
for AI collaborators. Code chunks are entity type 1, hunches are type 2.
Scars and session state (tears) are next. The name was always right.
"""

# Hunches - soft observations indexed by cqs
# These surface in search results when semantically relevant.

[[hunch]]
date = "2026-01-31"
title = "tree-sitter version gap"
severity = "med"
confidence = "med"
resolution = "open"
mentions = ["tree-sitter", "Cargo.toml"]
description = """
Grammar crates (0.23-0.25) have dev-dep on tree-sitter ^0.23, but we're using
tree-sitter 0.26. Works via tree-sitter-language abstraction layer. If parsing
breaks mysteriously, check this first.

Updated 2026-01-31: Bumped grammars to reduce gap. Still not fully aligned
with tree-sitter 0.26 but closer. All tests pass.
"""

[[hunch]]
date = "2026-01-31"
title = "ort 2.x is still RC"
severity = "med"
confidence = "high"
resolution = "open"
mentions = ["ort", "Cargo.toml", "embedder.rs"]
description = """
Using ort = "2.0.0-rc.11" - no stable 2.0 release yet. API could change.
Pin exact version and watch for breaking changes on upgrade.

Updated 2026-01-31: Still on rc.11. Dependabot will notify when stable releases.
No API issues encountered so far.
"""

[[hunch]]
date = "2026-01-31"
title = "WSL /mnt/c/ permission hell"
severity = "low"
confidence = "high"
resolution = "open"
mentions = [".cargo/config.toml", "libsqlite3-sys"]
description = """
Building Rust on Windows paths from WSL causes random permission errors
(libsqlite3-sys, git config). Workaround in place (.cargo/config.toml),
but might bite us elsewhere.
"""

[[hunch]]
date = "2026-01-31"
title = "r2d2 pool size may need tuning"
severity = "low"
confidence = "low"
resolution = "open"
mentions = ["store.rs", "r2d2"]
description = """
Added r2d2-sqlite with max 4 connections. This is arbitrary. For CPU-bound
embedding work, more connections don't help (bottleneck is GPU/CPU, not DB).
For pure search workloads (parallel queries), more connections could help.
Monitor if users report connection pool exhaustion errors.
"""

[[hunch]]
date = "2026-01-31"
title = "ONNX model input/output assumptions"
severity = "high"
confidence = "high"
resolution = "resolved"
mentions = ["embedder.rs", "nomic-embed-text"]
description = """
nomic-embed-text-v1.5 ONNX model needs: i64 inputs (not i32), token_type_ids
(all zeros), and outputs last_hidden_state (not sentence_embedding). These
aren't obvious from docs. If switching models, verify inputs/outputs with
the model directly - don't assume they're standard.
"""

[[hunch]]
date = "2026-01-31"
title = "hnsw_rs lifetime design forces reload on search"
severity = "low"
confidence = "high"
resolution = "open"
mentions = ["hnsw.rs", "hnsw_rs"]
description = """
The hnsw_rs crate returns Hnsw<'a> with lifetime tied to HnswIo. Can't store
loaded index and search later without lifetime issues. Workaround: store path
info in HnswInner::Loaded, reload on each search. Works but adds ~1-2ms
overhead. Watch for library updates that fix this.
"""

[[hunch]]
date = "2026-01-31"
title = "NL descriptions are shorter than raw code"
severity = "low"
confidence = "med"
resolution = "open"
mentions = ["nl.rs", "embedder.rs"]
description = """
The NL template produces ~50-100 chars vs 500+ for raw code. nomic-embed-text
was trained on longer texts. Might affect embedding quality. Monitor recall@5
on eval suite - if it drops, consider adding body tokens back.
"""

[[hunch]]
date = "2026-02-01"
title = "No schema migrations, just rebuild"
severity = "med"
confidence = "high"
resolution = "open"
mentions = ["store.rs", "schema.sql"]
description = """
When schema version bumps (v3→v4→v5→v6), users must run `cqs index --force`
to rebuild. No incremental migrations. Acceptable for now since reindexing
is fast (<30s for most projects), but could be painful for large codebases.
"""

[[hunch]]
date = "2026-02-01"
title = "Call graph method ambiguity"
severity = "low"
confidence = "high"
resolution = "open"
mentions = ["store.rs", "parser.rs"]
description = """
store.search() and hnsw.search() both match callee name "search". No type
information in call graph - can't distinguish which search is called. Would
need type analysis to resolve. Documented limitation, won't fix soon.
"""

[[hunch]]
date = "2026-02-01"
title = "bincode deserialization risk in HNSW"
severity = "high"
confidence = "high"
resolution = "resolved"
mentions = ["hnsw.rs", "hnsw_rs", "bincode"]
description = """
hnsw_rs uses bincode 1.3.3 (unmaintained, RUSTSEC-2025-0141) to serialize
the HNSW index. We load these files without checksum verification.

Threat: attacker crafts malicious .cq/hnsw/* files, user runs cqs search,
bincode deserializes → potential RCE or memory corruption.

Mitigating factors: index files are in user-controlled .cq/ directory,
not downloaded from network. Attacker needs local write access.

Fix options:
1. Add blake3 checksum to HNSW save/load (like we do for model files)
2. File issue on hnsw_rs to switch bincode → postcard/rmp-serde
3. Fork hnsw_rs
"""

[[hunch]]
date = "2026-02-01"
title = "Natural language carries valence"
severity = "high"
confidence = "high"
resolution = "accepted"
mentions = ["scar.rs", "embedder.rs"]
description = """
Don't need explicit pain/gain fields. Words like "failed", "broke", "wasted"
carry negative valence. Words like "worked", "clean", "saved" carry positive.
The embedding model learned this from training data. Schema can simplify to
just 'text' field - the semantics encode the sentiment.
"""

[[hunch]]
date = "2026-02-01"
title = "769th dimension for valence"
severity = "med"
confidence = "med"
resolution = "open"
mentions = ["embedder.rs", "store.rs", "hnsw.rs"]
description = """
Could append a valence float (-1 to +1) as 769th embedding dimension.
Similarity search then weights by sentiment automatically. "What went wrong"
queries would naturally surface negative-valence notes. Requires dimension
change across all embedding storage (768 → 769), breaking schema change.
"""

[[hunch]]
date = "2026-02-01"
title = "Git as team sync layer"
severity = "med"
confidence = "high"
resolution = "accepted"
mentions = ["scars.toml", "tears"]
description = """
Team knowledge sharing needs no infrastructure. Commit scars.toml to repo.
Git push/pull handles sync. Access control = repo permissions. History = blame.
Conflict resolution = merge. Async team memory without building anything new.
"""

[[hunch]]
date = "2026-02-01"
title = "Locality is the feature"
severity = "high"
confidence = "high"
resolution = "accepted"
mentions = ["CLAUDE.md", "tears"]
description = """
cqs indexes YOUR codebase. Scars are YOUR team's memory. Not portable,
not general-purpose - that's the design. Context persistence for AI
collaborators is inherently local. The value is specificity to one project,
one workflow, one history. "Personal tool" is the point, not a limitation.
"""

[[hunch]]
date = "2026-02-01"
title = "cuVS CAGRA for GPU-native search"
severity = "med"
confidence = "high"
resolution = "open"
mentions = ["hnsw.rs", "Cargo.toml"]
description = """
NVIDIA cuVS with CAGRA algorithm: 8-12x faster index builds, 4-8x faster
search vs CPU HNSW. Rust bindings exist (cuvs crate). Requires CUDA + libcuvs
system library. Feature-flagged as gpu-search. For GPU-equipped infrastructure,
this is the path. hnsw_rs remains default for CPU-only users.
"""

[[hunch]]
date = "2026-02-01"
title = "Prediction errors are the sparse signal"
severity = "med"
confidence = "high"
resolution = "accepted"
mentions = ["scar.rs", "tears"]
description = """
Scars (pain) and wins (gain) are both prediction errors - where the model
expected X but got Y. That's why they're high-value: they mark where
understanding needs updating. Don't index everything, index where predictions
failed. Sparse, high-signal edge of knowledge.
"""

[[hunch]]
date = "2026-02-01"
title = "Scale changes economics"
severity = "med"
confidence = "high"
resolution = "accepted"
mentions = ["embedder.rs", "cli.rs"]
description = """
90% savings at 1 instance = nice. 90% savings at 1M instances = $90k/month.
cqs as local tool has different economics than cqs as MCP server at scale.
Watch mode, embedding efficiency, GPU utilization all matter more when
multiplied by thousands of concurrent users.
"""

[[hunch]]
date = "2026-02-01"
title = "cqs is MCP server first, CLI second"
severity = "high"
confidence = "high"
resolution = "accepted"
mentions = ["mcp.rs", "cli.rs"]
description = """
Primary consumer is Claude via MCP tools, not human via CLI. Design for AI
consumption: notes formatted for embedding, search results for parsing,
cqs_add_note for writes. Human only touches for setup (init, watch, serve).
The whole thing is Claude infrastructure.
"""

[[hunch]]
date = "2026-02-01"
title = "Fleet model: append-only with curator impulse"
severity = "med"
confidence = "med"
resolution = "open"
mentions = ["notes.toml", "mcp.rs"]
description = """
Multiple Claudes share notes.toml via Git. Anyone appends via cqs_add_note.
No dedicated curator agent - curation is an impulse any Claude can have.
Append-only avoids merge conflicts. Truth emerges from accumulation.
Old notes get contradicted by new notes. Natural selection.
"""
